# -*- coding: utf-8 -*-
"""bbb_INDparam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lisf8zNfIhX5V-0s1ZmlyeUQV1aTMdUP
"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, LeaveOneOut, train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import RidgeClassifier, Perceptron
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
# from catboost import CatBoostClassifier
import numpy as np

pos=pd.read_csv('/content/posbbb.csv')
neg=pd.read_csv('/content/negbbb.csv')
df = pd.concat([pos,neg], axis=0)
df=df.reset_index(drop=True)

from sklearn.preprocessing import StandardScaler,MinMaxScaler
#dataset = pd.read_csv('df.csv', sep=',')
dataset = df
X = dataset.drop(['class'],axis=1)
y = dataset['class']
X = X.to_numpy()
y = y.to_numpy()
std_scale = MinMaxScaler().fit(X)
X = std_scale.transform(X)
X = np.nan_to_num(X.astype('float32'))

from xgboost import XGBClassifier

from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.ensemble import ExtraTreesClassifier
from lightgbm import LGBMClassifier

# Define the parameter grid for each classifier
xgb_param_grid = {
    'n_estimators': [100, 200, 300,400,500],
    'max_depth': [3, 5, 7,9],
    'learning_rate': [0.1, 0.01, 0.001]
}

extra_trees_param_grid = {
    'n_estimators': [100, 200, 300,500,400],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

lgbm_param_grid = {
    'n_estimators': [100, 200, 300,400,500],
    'max_depth': [3, 5, 7,9],
    'learning_rate': [0.1, 0.01, 0.001]
}

# Create the classifiers
xgb_classifier = XGBClassifier()
extra_trees_classifier = ExtraTreesClassifier()
lgbm_classifier = LGBMClassifier()

# Perform GridSearchCV for each classifier
xgb_grid_search = RandomizedSearchCV(xgb_classifier, xgb_param_grid, cv=5)
extra_trees_grid_search = RandomizedSearchCV(extra_trees_classifier, extra_trees_param_grid, cv=5)
lgbm_grid_search = RandomizedSearchCV(lgbm_classifier, lgbm_param_grid, cv=5)

# Fit the data to perform hyperparameter tuning
xgb_grid_search.fit(X, y)
extra_trees_grid_search.fit(X, y)
lgbm_grid_search.fit(X, y)

# Get the best parameters and best score for each classifier
xgb_best_params = xgb_grid_search.best_params_
xgb_best_score = xgb_grid_search.best_score_

extra_trees_best_params = extra_trees_grid_search.best_params_
extra_trees_best_score = extra_trees_grid_search.best_score_

lgbm_best_params = lgbm_grid_search.best_params_
lgbm_best_score = lgbm_grid_search.best_score_

# Print the results
print("XGBClassifier:")
print("Best Parameters:", xgb_best_params)
print("Best Score:", xgb_best_score)
print()

print("ExtraTreesClassifier:")
print("Best Parameters:", extra_trees_best_params)
print("Best Score:", extra_trees_best_score)
print()

print("LGBMClassifier:")
print("Best Parameters:", lgbm_best_params)
print("Best Score:", lgbm_best_score)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.234)

from xgboost import XGBClassifier
from sklearn.ensemble import ExtraTreesClassifier
from lightgbm import LGBMClassifier

# Create the classifiers with the best parameters
xgb_classifier = XGBClassifier(n_estimators=400, max_depth=9, learning_rate=0.1)
extra_trees_classifier = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2)
lgbm_classifier = LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.1)

# Fit the models with the best parameters
xgb_classifier.fit(X_train, y_train)
extra_trees_classifier.fit(X_train, y_train)
lgbm_classifier.fit(X_train, y_train)

# Make predictions on the test set
xgb_predictions = xgb_classifier.predict(X_test)
extra_trees_predictions = extra_trees_classifier.predict(X_test)
lgbm_predictions = lgbm_classifier.predict(X_test)

# Evaluate the models
xgb_accuracy = accuracy_score(y_test, xgb_predictions)
extra_trees_accuracy = accuracy_score(y_test, extra_trees_predictions)
lgbm_accuracy = accuracy_score(y_test, lgbm_predictions)

# Print the accuracies
print("XGBClassifier Accuracy:", xgb_accuracy)
print("ExtraTreesClassifier Accuracy:", extra_trees_accuracy)
print("LGBMClassifier Accuracy:", lgbm_accuracy)

import pickle

# Save the XGBoost model
with open('xgb_modelup83.pkl', 'wb') as file:
    pickle.dump(xgb_classifier, file)

# Save the Extra Trees model
with open('extra_trees_modelup83.pkl', 'wb') as file:
    pickle.dump(extra_trees_classifier, file)

# Save the LGBM model
with open('lgbm_modelup83.pkl', 'wb') as file:
    pickle.dump(lgbm_classifier, file)

import pandas as pd

# Convert y_train and y_test arrays to dataframes
y_train_df = pd.DataFrame({'target': y_train})
y_test_df = pd.DataFrame({'target': y_test})

# Merge X_train and y_train dataframes
train_data = pd.concat([X_train, y_train_df], axis=1)

# Merge X_test and y_test dataframes
test_data = pd.concat([X_test, y_test_df], axis=1)

# Save train_data dataframe as CSV file
train_data.to_csv('train_data77up.csv', index=False)

# Save test_data dataframe as CSV file
test_data.to_csv('test_data23up.csv', index=False)

from google.colab import files

# Download train_data.csv
files.download('train_data77up.csv')

# Download test_data.csv
files.download('test_data23up.csv')